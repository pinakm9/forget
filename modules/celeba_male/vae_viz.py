import torch, math
import numpy as np
from vae import VAE
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import os, sys, glob, json
from torchvision.utils import save_image
import torchvision.utils as vutils

sys.path.append(os.path.abspath('../modules'))
import utility as ut
import classifier
import pandas as pd
import vae 
from PIL import Image


def generate_random_samples(model, num_samples=25, latent_dim=512):
    """
    Generate a grid of random images using a trained VAE model.

    Parameters
    ----------
    model : VAE or str
        The VAE model to use for generating images. Can be a VAE instance or
        a path to a saved model state dict.
    num_samples : int, optional
        The number of images to generate. Default is 25.
    latent_dim : int, optional
        The dimensionality of the latent space. Default is 512.

    Notes
    -----
    This function generates a grid of random images using the decoder of the
    VAE model. The images are sampled from a standard normal distribution in
    the latent space.
    """
    net = model
    net.eval()  # Set model to evaluation mode
    device = next(net.parameters()).device

    # Decode the latent vectors into images
    with torch.no_grad():
        z_random = torch.randn(num_samples, latent_dim).to(device)
        generated_images = model.decode(z_random)
    
    nrow = int(math.sqrt(num_samples))
    grid = vutils.make_grid(generated_images.cpu(), nrow=nrow, normalize=True)

    # Show the image
    plt.figure(figsize=(8, 8))
    plt.axis('off')
    plt.title('Generated Images')
    plt.imshow(grid.permute(1, 2, 0))  # Convert from [C,H,W] to [H,W,C]
    plt.show()
    return generated_images




def compare_model_outputs(model_a, model_b, num_samples=25, save_path=None, latent_dim=512, image_size=64):
    """
    Compare CelebA images generated by two models using the same random latent samples.

    Parameters
    ----------
    model_a : VAE
        First VAE model to use for generating images.
    model_b : VAE
        Second VAE model to use for generating images.
    num_samples : int, optional
        The number of images to generate. Default is 25.
    save_path : str, optional
        Path to save the noise vectors. Default is None.
    latent_dim : int, optional
        The dimensionality of the latent space. Default is 512.
    image_size : int, optional
        Size of the output image (assumes square and 3 channels). Default is 64.
    """
    model_a.eval()
    model_b.eval()

    device_a = next(model_a.parameters()).device
    device_b = next(model_b.parameters()).device


    try:
        z_random = torch.tensor(np.load(save_path), dtype=torch.float32)[:num_samples]
    except:
        z_random = torch.randn(num_samples, latent_dim)
    
    z_a = z_random.to(device_a)
    z_b = z_random.to(device_b)

    if save_path is not None:
        np.save(save_path, z_random.cpu().numpy())

    with torch.no_grad():
        images_a = model_a.decode(z_a).cpu().numpy()
        images_b = model_b.decode(z_b).cpu().numpy()

    # Convert shape from (N, 3, H, W) to (N, H, W, 3)
    images_a = images_a.transpose(0, 2, 3, 1)
    images_b = images_b.transpose(0, 2, 3, 1)

    # Normalize images to [0, 1]
    images_a = (images_a - images_a.min()) / (images_a.max() - images_a.min())
    images_b = (images_b - images_b.min()) / (images_b.max() - images_b.min())

    grid_size = int(num_samples ** 0.5)
    fig, axes = plt.subplots(2 * grid_size, grid_size, figsize=(grid_size * 2, grid_size * 4))

    for idx in range(num_samples):
        row = idx // grid_size
        col = idx % grid_size

        axes[2 * row, col].imshow(images_a[idx])
        axes[2 * row, col].set_title(f"Model A - {idx}")
        axes[2 * row, col].axis("off")

        axes[2 * row + 1, col].imshow(images_b[idx])
        axes[2 * row + 1, col].set_title(f"Model B - {idx}")
        axes[2 * row + 1, col].axis("off")

    plt.tight_layout()
    plt.show()

    return fig, axes

def compare_model_outputs_2(model_a, model_b, num_samples=25, save_path=None, latent_dim=512, image_size=64):
    """
    Compare CelebA images generated by two models using the same random latent samples.
    """

    model_a.eval()
    model_b.eval()

    device_a = next(model_a.parameters()).device
    device_b = next(model_b.parameters()).device

    # Load or generate latent vectors
    try:
        z_random = torch.tensor(save_path, dtype=torch.float32)[:num_samples]
    except:
        if save_path is not None:
            z_random = torch.tensor(np.load(save_path), dtype=torch.float32)[:num_samples]
        else:
            z_random = torch.randn(num_samples, latent_dim)

    z_a = z_random.to(device_a)
    z_b = z_random.to(device_b)

    # Decode latent vectors
    with torch.no_grad():
        images_a = model_a.decode(z_a).cpu().numpy()
        images_b = model_b.decode(z_b).cpu().numpy()

    # Convert and normalize images
    images_a = images_a.transpose(0, 2, 3, 1)
    images_b = images_b.transpose(0, 2, 3, 1)

    images_a = (images_a - images_a.min()) / (images_a.max() - images_a.min())
    images_b = (images_b - images_b.min()) / (images_b.max() - images_b.min())

    # Combine A and B into a single list alternating
    images = []
    titles = []
    for i in range(num_samples):
        images.append(images_a[i])
        titles.append("Before")
        images.append(images_b[i])
        titles.append("After")

    total_images = len(images)  # 2 * num_samples
    grid_size = math.ceil(math.sqrt(total_images))

    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size * 2, grid_size * 2))
    axes = axes.flatten()

    for i in range(len(images)):
        axes[i].imshow(images[i])
        axes[i].set_title(titles[i], fontsize=18)
        axes[i].axis("off")

    # Hide any remaining unused axes
    for i in range(len(images), len(axes)):
        axes[i].axis("off")

    plt.tight_layout()
    plt.show()

    return fig, axes







def plot_transformation(model_a, model_b, z_random, save_path=None, latent_dim=512, image_size=64):
    """
    Compare CelebA images generated by two models using the same random latent samples.
    Displayed in two rows: top row for model_a, bottom row for model_b.

    Parameters
    ----------
    model_a : VAE
        First VAE model to use for generating images.
    model_b : VAE
        Second VAE model to use for generating images.
    z_random : np.ndarray or torch.Tensor
        Pre-sampled latent vectors.
    save_path : str, optional
        Path to save the noise vectors. Default is None.
    latent_dim : int, optional
        The dimensionality of the latent space. Default is 512.
    image_size : int, optional
        Size of the output image (assumes square and 3 channels). Default is 64.
    """
    model_a.eval()
    model_b.eval()

    num_samples = z_random.shape[0]

    device_a = next(model_a.parameters()).device
    device_b = next(model_b.parameters()).device

    z_random = torch.tensor(z_random, dtype=torch.float32)
    z_a = z_random.to(device_a)
    z_b = z_random.to(device_b)

    with torch.no_grad():
        images_a = model_a.decode(z_a).cpu().numpy()
        images_b = model_b.decode(z_b).cpu().numpy()

    images_a = images_a.transpose(0, 2, 3, 1)
    images_b = images_b.transpose(0, 2, 3, 1)

    images_a = (images_a - images_a.min()) / (images_a.max() - images_a.min())
    images_b = (images_b - images_b.min()) / (images_b.max() - images_b.min())

    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 2 + 1, 4))

    for idx in range(num_samples):
        axes[0, idx].imshow(images_a[idx])
        axes[0, idx].axis("off")

        axes[1, idx].imshow(images_b[idx])
        axes[1, idx].axis("off")


    # Add vertical labels to the left using fig.text
    fig.subplots_adjust(left=0.005, wspace=0.0, hspace=0.02)
    fig.text(0.0001, 0.7, "Original model", va='center', ha='left', rotation=90, fontsize=14)
    fig.text(0.0001, 0.3, "After unlearning", va='center', ha='left', rotation=90, fontsize=14)

    # Top labels for gender
    fig.text(0.23, 0.9, "Male", ha='center', va='bottom', fontsize=14)
    fig.text(0.67, 0.9, "Female", ha='center', va='bottom', fontsize=14)

    if save_path is not None:
        plt.savefig(save_path, bbox_inches="tight")
    plt.show()

    return fig, axes












def find_closest_image_in_dataset(model, feature_path='../../data/CelebA/vae/features_200.npy',\
    image_dir='../../data/CelebA/dataset-reconstructed/img_align_celeba_reconstructed'):
    device = next(model.parameters()).device
    features = torch.tensor(np.load(feature_path)).to(device)
    features = features.view(features.shape[0], -1)
    model.eval()

    # Generate an image
    gen_img = model.decode(torch.randn(1, 512).to(device))
    # print("Generated image shape:", gen_img.shape)

    # Encode the generated image to get its features
    feat = model.encoder(gen_img)
    feat = feat.view(feat.shape[0], -1)  # Shape: (1, feature_dim)
    # print("Generated feature shape:", feat.shape, "Dataset feature shape:", features.shape)

    # Compute distances
    dists = torch.norm(features - feat, dim=1)  # Euclidean distance
    closest_idx = torch.argmin(dists).item()
    print("Closest index:", closest_idx)

    # Load the closest image from disk
    filename = f"{closest_idx:06d}.jpg"  # zero-padded to 6 digits
    img_path = os.path.join(image_dir, filename)
    closest_img = Image.open(img_path).convert("RGB")
    
    # Plot the generated and closest image
    fig, axes = plt.subplots(1, 2, figsize=(8, 4))
    gen_img_np = gen_img.squeeze().detach().cpu().numpy()

    if gen_img_np.ndim == 3:
        gen_img_np = np.transpose(gen_img_np, (1, 2, 0))  # C, H, W -> H, W, C
        # closest_img_np = np.transpose(closest_img_np, (1, 2, 0))

    axes[0].imshow(gen_img_np.clip(0, 1))
    axes[0].set_title("Generated Image")
    axes[0].axis("off")

    axes[1].imshow(closest_img)
    axes[1].set_title("Closest Image")
    axes[1].axis("off")

    plt.tight_layout()
    plt.show()

    return closest_idx




@ut.timer
def evolve(model, folder, num_samples=25, fps=12, total_frames=100, latent_dim=512):
    """
    Creates an animation showing the evolution of generated color images over different model checkpoints.

    Parameters:
        model: The generative model with a `.decoder` method.
        folder (str): Base folder containing 'checkpoints' where model states are stored.
        num_samples (int): Number of images to generate per frame.
        fps (int): Frames per second for the final animation.
    """
    device = next(model.parameters()).device
    checkpoints_dir = os.path.join(folder, "checkpoints")
    pth_files = sorted(glob.glob(os.path.join(checkpoints_dir, "*.pth")))
    if len(pth_files) < total_frames:
        total_frames = len(pth_files)

    samples_dir = os.path.join(folder, "samples")
    os.makedirs(samples_dir, exist_ok=True)

    grid_size = int(np.ceil(np.sqrt(num_samples)))
    z_random = torch.randn(num_samples, latent_dim).to(device)

    # Create figure and axes for animation
    fig, axes = plt.subplots(grid_size, grid_size, figsize=(8, 8))
    if grid_size == 1:
        axes = np.array([axes])
    else:
        axes = axes.flatten()

    # Initialize the image plots with blank color images
    blank_image = np.zeros((64, 64, 3))  # Assuming 64x64 RGB
    image_plots = [ax.imshow(blank_image, animated=True) for ax in axes]
    for ax in axes:
        ax.axis("off")

    title = fig.suptitle("Fraction of males = ?")

    def update(frame_idx):
        model.load_state_dict(torch.load(pth_files[frame_idx], map_location=device))
        with torch.no_grad():
            generated_images = model.decoder(z_random)

        num_ones = classifier.count_male(generated_images, 1, device=device)

        # Clamp and convert to numpy
        generated_images = generated_images.clamp(0, 1).cpu().numpy()
        generated_images = np.transpose(generated_images, (0, 2, 3, 1))  # CHW -> HWC

        for i, img_plot in enumerate(image_plots):
            if i < num_samples:
                img_plot.set_array(generated_images[i])

        title.set_text(f"Fraction of males = {num_ones / num_samples:.3f}")
        return image_plots + [title]

    ani = animation.FuncAnimation(fig, update, frames=np.linspace(0, len(pth_files)-1, total_frames, dtype=int), interval=1000//fps, blit=False)

    output_video = os.path.join(samples_dir, "evolution.mp4")
    ani.save(output_video, writer="ffmpeg", fps=fps)

    print(f"Animation saved as {output_video}")



def evolve(folder, window=1):
    """
    Plots the evolution of Total Loss, Orthogonality Loss, Uniformity Loss, Fraction of forget class, Margin, and Image Quality metrics over the training steps.

    Parameters:
        folder (str): The base folder containing the `checkpoints` directory with the training log.
        window (int, optional): The rolling window size for the moving average of the loss and metrics. Default is 1.

    Notes:
        This function assumes that the training log is in the `checkpoints` directory inside the provided folder.
        The plot is saved as `evolution.png` in the same folder.
    """
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    data = pd.read_csv(f"{folder}/checkpoints/training_log.csv")
    config = ut.get_config(folder)
    # window = 1#int(max(1, config["training"]["epoch_length"]["value"] / config["experiment"]["log_interval"]["value"]))

    # plot Total Loss vs Step
    axes[0, 0].semilogy(data["Step"], data["Total Loss"].rolling(window).mean())
    axes[0, 0].set_ylabel("Total Loss")


    # plot Orthogonality Loss vs Step
    try:
        axes[0, 1].semilogy(data["Step"], data["Orthogonality Loss"].rolling(window).mean())
        axes[0, 1].set_ylabel("Orthogonality Loss")
    except: 
        pass
    
    # plot Uniformity Loss vs Step
    try:
        axes[0, 2].semilogy(data["Step"], data["Uniformity Loss"].rolling(window).mean())
        axes[0, 2].set_ylabel("Uniformity Loss")
    except: 
        pass
    
    try:
        # plot fraction of forget class vs Step
        forget_class = config['experiment']['forget_class']["value"]
        axes[1, 0].plot(data["Step"], data[f"{forget_class} Fraction"])
        axes[1, 0].set_ylabel(f"Fraction of {forget_class}s")
        axes[1, 0].set_xlabel("Step")
    except:
        pass

    try:
        # plot Margin vs Step
        axes[1, 1].plot(data["Step"], data["Margin"])
        axes[1, 1].set_ylabel("Margin")
    except:
        pass

    try:
        # plot quality vs Step
        axes[1, 2].plot(data["Step"], data["FID"], label="FID")
        axes[1, 2].plot(data["Step"], data["IS"], label="IS")
        axes[1, 2].set_ylabel("Image Quality")
        axes[1, 2].legend()
    except:
        pass


    fig.supxlabel("Step")
    plt.savefig(f"{folder}/evolution.png", bbox_inches="tight") 






@ut.timer
def summarize_training(folder, window=1, total_duration=15):
    """
    Summarize the training run by plotting the evolution of metrics and saving an animation of the generated samples.

    Parameters:
        folder (str): The base folder containing the `checkpoints` directory with the training log.
        window (int, optional): The rolling window size for the moving average of the loss and metrics. Default is 1.
        total_duration (int or None, optional): The total duration of the video in seconds. If None, no video is saved. Defaults to 15.

    Notes:
        This function assumes that the training log is in the `checkpoints` directory inside the provided folder.
        The plot is saved as `evolution.png` in the same folder, and the animation is saved as `sample_evolution.mp4` in the `samples` folder.
    """
    evolve(folder, window)
    config = ut.get_config(folder)
    if isinstance(total_duration, int):
        ut.stitch(f"{folder}/samples", config["experiment"]["img_ext"]["value"], f"{folder}/samples/sample_evolution.mp4", total_duration, delete_images=True)
