{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dda146e-1480-46a0-9804-ff340d9d252b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8164d827874222ae44ab3ea58df789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:     675.13M\n",
      "Trainable params: 1.15M  (y: 1.15M, adaLN_last0: 0.00M, final: 0.00M)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pman0581/Documents/GitHub/forget/practice/train.py:154: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"../../fast-DiT\")\n",
    "sys.path.append(\"../modules\")\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import dit \n",
    "import utility as ut\n",
    "import imagenet_maps as imap\n",
    "import datapipe as dp\n",
    "import classifier as cls\n",
    "import train as tt\n",
    "import csv\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model, vae = dit.load_DiT(folder=\"../data/ImageNet-1k/DiT-XL-2\", device=device)\n",
    "diffusion = dit.load_diffusion(64)\n",
    "exchange_classes = [208]\n",
    "forget_class = 207\n",
    "JSON = \"../data/ImageNet-1k/imagenet_class_index.json\"\n",
    "root = \"../data/ImageNet-1k/2012\"\n",
    "gen_kwargs = {\"cfg_scale\": 10., \"n_samples\": 10}\n",
    "save_steps = [10, 20, 40]\n",
    "checkpoint_dir = '.'\n",
    "epoch_length = 20\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "identifier = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\").to(device)\n",
    "model.train()\n",
    "model, _ = dit.freeze_except_y_and_lastK_adaln(model, K=0)\n",
    "\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=1e-3)\n",
    "process_batch = tt.get_processor(model, vae, diffusion, device, optim, exchange_classes, forget_class)\n",
    "dl = dp.get_dataloader(root, 208, JSON, batch_size=1)\n",
    "\n",
    "\n",
    "csv_file = f\"training_log.csv\"\n",
    "header = [\"Step\", \"Total Loss\", \"Time\"]\n",
    "header += [f\"{i} Fraction\" for i in range(2)]\n",
    "# Write CSV header\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d08adf-7478-44c2-a490-9e328e78b5fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff7251ec943482e86591cfbb593563e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_results = tt.get_logger(model, vae, diffusion, identifier, csv_file, 1, 207, **gen_kwargs)\n",
    "log_results(17, [1e-2], 1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed55dd2-d1d9-498d-87a7-21f10def83bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pman0581/miniconda3/envs/uno/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/pman0581/Documents/GitHub/forget/practice/train.py:164: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=amp):\n",
      "/Users/pman0581/miniconda3/envs/uno/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for step, (imgs, targets) in enumerate(dl):\n",
    "    imgs = imgs.to(device, non_blocking=True)\n",
    "    targets = targets.to(device, non_blocking=True)  # all the same class for single-WNID\n",
    "    process_batch(imgs, targets)\n",
    "    i += 1\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4950917e-46f5-4560-8d77-bb39d7d4ba0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fedc1a618ae47d8a0ca621c9acf3977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.08 GiB, other allocations: 2.51 GiB, max allowed: 18.13 GiB). Tried to allocate 5.00 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model.eval()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m imgs = \u001b[43mdit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m208\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/forget/practice/dit.py:186\u001b[39m, in \u001b[36mgenerate_cfg\u001b[39m\u001b[34m(model, vae, diffusion, class_id, n_samples, cfg_scale, device, show)\u001b[39m\n\u001b[32m    180\u001b[39m     latents = diffusion.p_sample_loop(\n\u001b[32m    181\u001b[39m         model.forward_with_cfg, z.shape, z,\n\u001b[32m    182\u001b[39m         clip_denoised=\u001b[38;5;28;01mFalse\u001b[39;00m, model_kwargs=\u001b[38;5;28mdict\u001b[39m(y=y, cfg_scale=cfg_scale),\n\u001b[32m    183\u001b[39m         progress=\u001b[38;5;28;01mTrue\u001b[39;00m, device=device\n\u001b[32m    184\u001b[39m     )\n\u001b[32m    185\u001b[39m     sample = latents / \u001b[32m0.18215\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     imgs = \u001b[43mvae\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m.sample  \u001b[38;5;66;03m# (2n_samples,3,256,256)\u001b[39;00m\n\u001b[32m    187\u001b[39m     imgs = imgs[:n_samples]\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m show:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# visualize a grid\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[39m, in \u001b[36mapply_forward_hook.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_hf_hook\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._hf_hook, \u001b[33m\"\u001b[39m\u001b[33mpre_forward\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m._hf_hook.pre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:323\u001b[39m, in \u001b[36mAutoencoderKL.decode\u001b[39m\u001b[34m(self, z, return_dict, generator)\u001b[39m\n\u001b[32m    321\u001b[39m     decoded = torch.cat(decoded_slices)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     decoded = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m.sample\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (decoded,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/diffusers/models/autoencoders/autoencoder_kl.py:294\u001b[39m, in \u001b[36mAutoencoderKL._decode\u001b[39m\u001b[34m(self, z, return_dict)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.post_quant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    292\u001b[39m     z = \u001b[38;5;28mself\u001b[39m.post_quant_conv(z)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m dec = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (dec,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/diffusers/models/autoencoders/vae.py:305\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, sample, latent_embeds)\u001b[39m\n\u001b[32m    303\u001b[39m     \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m up_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.up_blocks:\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m         sample = \u001b[43mup_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_embeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;66;03m# post-process\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m latent_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/diffusers/models/unets/unet_2d_blocks.py:2643\u001b[39m, in \u001b[36mUpDecoderBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, temb)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2642\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers:\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m         hidden_states = \u001b[43mupsampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/diffusers/models/upsampling.py:177\u001b[39m, in \u001b[36mUpsample2D.forward\u001b[39m\u001b[34m(self, hidden_states, output_size, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m     hidden_states = hidden_states.contiguous()\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     hidden_states = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnearest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    179\u001b[39m     hidden_states = F.interpolate(hidden_states, size=output_size, mode=\u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/uno/lib/python3.12/site-packages/torch/nn/functional.py:4730\u001b[39m, in \u001b[36minterpolate\u001b[39m\u001b[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[39m\n\u001b[32m   4728\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[32m   4729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4730\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   4732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 9.08 GiB, other allocations: 2.51 GiB, max allowed: 18.13 GiB). Tried to allocate 5.00 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "imgs = dit.generate_cfg(model, vae, diffusion, 208, 40, 2, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fce39c-7a93-4f61-8a63-3cc10303519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a552f3-609d-4f49-843d-942039fd0eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
