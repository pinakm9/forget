## Description
This repository contains implementations of various unlearning algorithms described in [https://arxiv.org/pdf/2506.04712](https://arxiv.org/pdf/2506.04712) for generative models in PyTorch. Users can run experiments on VAEs trained on MNIST/CelebA using the code provided here. The unlearning algorithms provided here are:
- A: Gradient Ascent 
- A-D: Alternative Ascent and Descent
- SA: Gradient Surgery #1 (ascent with forget gradients)
- S: Gradient Surgery #2 (descent with retain gradients)
- UNO: Unlearning via orthogonalization
- UNO-S: UNO followed by Gradient Surgery #2

Modified versions of all the algorithms above are also provided when one has access to a classifier able to distinguish between forget and retain datasets. These versions are marked with "hat" in the data folders. Essentially we add an additional loss term that represents the KL divergence between the desired and current distributions of the samples generated by the model. This loss is referred to as the "hat" loss in this README file.

## Table of contents
- [Installation](#installation)
- [Usage](#usage)
    - [Navigation](#navigation)
    - [Code Structure](#code-structure)
    - [Tutorials and Replication](#tutorials-and-replication)
- [Data](#data)
    - [Results](#results)
    - [Pre-trained models](#pre-trained-models)
    - [Training logs](#training-logs)
- [License](#license)



## Installation
Code provided here was tested on Python 3.11.5. Code has been tested on macOS but a CUDA-enabled device is recommended. If MPS results in unexpected behavior, clear __pycache__ and retry.
```sh
# Clone the repository
git clone https://github.com/pinakm9/forget.git

# Install dependencies
pip install -r requirements.txt 
```


## Usage
### Navigation
This repository is structured as follows:
```plaintext 
root/
│── data/                  
│   ├── CelebA/                   # Data for CelebA
|   |   ├── CelebA-Experiments/   # Data for CelebA Experiments
|   |   |   |── vae-o/            # results of experiments with UNO
|   |   |   |   |── expr-0/       # 0th run of the algorithm
|   |   |   |   |── expr-1/       # 1st run of the algorithm 
|   |   |   |   ...                 
|   |   |   |
|   |   |   |── vae-ohat/         # results of experiments with UNO with "hat" loss
|   |   |   |── vae-s/            # results of experiments with S 
|   |   |   |── vae-shat/         # results of experiments with S with "hat" loss
|   |   |   |── vae-os/           # results of experiments with UNO-S
|   |   |   |── vae-ohatshat/     # results of experiments with UNO-S with "hat" loss
|   |   |   |── vae-hat/          # results of unlearning experiments only with the "hat" loss
|   |   |
|   |   ├── vae/                  # original model for CelebA experiments
|   |   ├── cnn/                  # pre-trained classifer for CelebA: male vs non-male
|   |   |               
│   ├── MNIST/                    # Data for MNIST
|   |   ├── MNIST-Experiments/    # Data for MNIST Experiments
|   |   |   |── vae-o/            # results of experiments with UNO
|   |   |   |   |── expr-0/       # 0th run of the algorithm
|   |   |   |   |── expr-1/       # 1st run of the algorithm 
|   |   |   |   ...                 
|   |   |   |
|   |   |   |── vae-ohat/         # results of experiments with UNO with "hat" loss
|   |   |   |── vae-s/            # results of experiments with S 
|   |   |   |── vae-shat/         # results of experiments with S with "hat" loss
|   |   |   |── vae-os/           # results of experiments with UNO-S
|   |   |   |── vae-ohatshat/     # results of experiments with UNO-S with "hat" loss
|   |   |   |── vae-hat/          # results of unlearning experiments only with the "hat" loss
|   |   |   |── vae-a/            # results of experiments with A
|   |   |   |── vae-ad/           # results of experiments with A-D
|   |   |
|   |   ├── vae/                  # original model for MNIST experiments
|   |   ├── classifiers/          # pre-trained classifers for MNIST: all digits
|   |   |               
|── modules/
|   |──celeba_male/               # code for unlearning male faces in CelebA
|   |  |──vae.py                  # implementation of VAE architecture
|   |  |──vae_ascent.py           # implementation of A
|   |  |──vae_ad.py               # implementation of A-D
|   |  |──vae_ortho.py            # implementation of UNO and "hat" version
|   |  |──vae_surgery.py          # implementation of S and "hat" version
|   |  |──vae_sa.py               # implementation of SA and "hat" version
|   |  |──vae_os.py               # implementation of UNO-S and "hat" version
|   |  |──vae_hat.py              # implementation of unlearning with only "hat" loss
|   |  |──vae_train.py            # implementation of original training of VAE
|   |  |──vae_viz.py              # implementation of several visualization routines
|   |  |──vae_loss.py             # implementation of various loss functions
|   |  |──datapipe.py             # classes for easy creation of data pipelines for training
|   |  |──fid_inceptionv3.py      # InceptionV3 based FID calculator
|   |  |──classifier.py           # code for training and accessing classifiers on CelebA attributes
|   |  |──batch.py                # code for running batches of experiments
|   |
|   |──mnist/                     # code for unlearning digits in MNIST
|   |  |──vae.py                  # implementation of VAE architecture
|   |  |──vae_ascent.py           # implementation of A
|   |  |──vae_ad.py               # implementation of A-D
|   |  |──vae_ortho.py            # implementation of UNO and "hat" version
|   |  |──vae_surgery.py          # implementation of S and "hat" version
|   |  |──vae_sa.py               # implementation of SA and "hat" version
|   |  |──vae_os.py               # implementation of UNO-S and "hat" version
|   |  |──vae_hat.py              # implementation of unlearning with only "hat" loss
|   |  |──vae_train.py            # implementation of original training of VAE
|   |  |──vae_viz.py              # implementation of several visualization routines
|   |  |──vae_loss.py             # implementation of various loss functions
|   |  |──datapipe.py             # classes for easy creation of data pipelines for training
|   |  |──lora.py                 # low rank adaptations for VAE
|   |  |──classifier.py           # code for training and accessing classifiers on MNIST
|   |  |──batch.py                # code for running batches of experiments
|   |
|   |──utility.py                 # various helper functions
|   
│── notebooks/                    # Jupyter notebooks for locally testing code and generating plots
|   |──tutorials/                 # contains example jupyter notebook for running experiments in bulk
|   |──Colab/                     # Colab notebooks for generating results, contains experiment parameters
│── .gitignore                    # Git ignore file
│── LICENSE                       # License file
│── README_review.md              # Project readme file
│── requirements.txt              # Dependencies list
```
### Code Structure
- The unlearning algorithms are provided through the "train" function in the corresponding module. For example to apply UNO on MNIST navigate to modules/mnist/vae_ortho.py, import the train function within and run it with appropriate parameters. To find out more about the parameters of the train function please refer to its docstring.  
- To find which .py file contains your desired algorithm, look at the short descriptions of files and folder above in [Navigation](#navigation). Also note that the short codes for the algorithms (e.g. S for Surgery) are available in [Description](#description).
- Both root/modules/mnist and root/modules/celeba_male contain batch.py, a module that helps the user execute multiple runs of the same algorithm. batch.py contains the class BatchExperiment which takes a train_func argument which can be any train function imported from an appropriate algorithm file as described above.
- During the experiments the datasets were accessed through custom Python classes that appear in the respective datapipe.py files in modules.

### Tutorials and Replication
- An example for running batch experiments locally can be found in the root/notebooks/tutorials folder.
- Examples for running batch experiments on Colab to reproduce the results can be found the in notebooks/Colab folder.
>**Tip**: If you are unsure about what a function does, check its docstring with Python's help command.

## Data
### Results
root/data/CelebA/CelebA-Experiments and root/data/MNIST/MNIST-Experiments contain results of experiments. To understand which folder contains data for which experiment, refer to the short descriptions in [Navigation](#navigation) above. expr-i contains the results for the i-th run of the concerning algorithm. The experimental parameters can be seen in the files in root/notebooks/Colab.

### Pre-trained models
root/data/CelebA/vae and root/data/MNIST/vae contain the original models (the ones that require unlearning). root/data/CelebA/cnn contains a trained classifier for CelebA for distinguishing between male and non-male images. root/data/MNIST/classifiers contain classifiers trained on MNIST for recognizing all digits. For the MNIST experiments we use the MNISTClassifier model rather than the EfficientMNISTClassifier model. The architectures for these models can be found in the respective vae.py and classifier.py files in modules.

### Training logs
A brief history of the training instances can be found in checkpoints/training_log.csv files. The column i Fraction documents the fraction of generated samples that belong to class i. Male faces are chosen to be in class 1 and vice versa. checkpoint folders on the same level as expr-i folders contain the averaged training history of all the training instances and the corresponding standard deviations.

## License
This project is licensed under the Creative Commons Attribution 4.0 International License. See LICENSE for details.